# Qwen3-Omni Service Configuration
# Environment variables for the vLLM service
# Copy this file to config.env and update with your system-specific paths

# Required for Qwen3-Omni compatibility
export VLLM_USE_V1=0

# CUDA Environment
# Update CUDA_HOME to match your CUDA installation path
export CUDA_HOME=/usr/local/cuda-12.9
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH

# Model Configuration
# Update MODEL_PATH to point to your Qwen3-Omni model directory
MODEL_PATH="/path/to/your/models/qwen3-omni-30b"
PORT=8002
HOST="0.0.0.0"
DTYPE="bfloat16"

# Performance Settings
# For videos up to 60s use 32768, for up to 120s use 65536
MAX_MODEL_LEN=32768
# Update TENSOR_PARALLEL_SIZE based on number of GPUs available
TENSOR_PARALLEL_SIZE=2  # Number of GPUs (e.g., 2x H100 GPUs)
GPU_MEMORY_UTIL=0.95
MAX_NUM_SEQS=8

# Generation Settings (from generation_config.json)
TEMPERATURE=0.6
TOP_P=0.95
TOP_K=20
MAX_TOKENS=16384







